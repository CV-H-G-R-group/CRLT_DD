<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<!-- <title>Abstract | CRLT_DD</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Abstract" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://cv-h-g-r-group.github.io/Long-tailed-Data-Distillation/" />
<meta property="og:url" content="https://cv-h-g-r-group.github.io/Long-tailed-Data-Distillation/" />
<meta property="og:site_name" content="CRLT_DD" />
<script type="application/ld+json">
{"@type":"WebSite","url":"https://cv-h-g-r-group.github.io/Long-tailed-Data-Distillation/","headline":"Abstract","name":"CRLT-DD","@context":"https://schema.org"}</script> -->
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="assets/css/style.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">CRLT_DD</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/CV-H-G-R-group/Long-tailed-Data-Distillation" class="btn">View on GitHub</a>
      
    </section>

    <section class="main-content">
      <h3 id="abstract">Abstract</h3>
<p>Long-tail datasets often result in models overfitting to the head classes (classes with many samples) and performing poorly on the tail classes.
In this paper, we aim to apply dataset distillation methods to long-tailed datasets for image classifier problems, which not only improves the training efficiency by reducing the data size but also effectively alleviates the long-tailed problem.
With this idea, we propose the strategy called Condensing and Rebalancing Long-Tailed datasets with Dataset Distillation (CRLT-DD), using the method called distribution matching for dataset distillation and adding self-supervised pre-train before classifying. We experiment with our approach on CIFAR10-LT and CIFAR100-LT and successfully prove our idea. Our code will be made public. </p>

<h3 id="author">Authors</h3>

<center class="half">
    <!-- <img style="margin: 0px 20px;" src="sjm.jpg" width="200" /> -->
    <img style="margin: 0px 20px;" src="gyj.jpg" width="200" />
</center>
<p>　　　　　　　Keya Hu　　　　　　　　Yijin Guo</p>

<h3 id="paper">Paper</h3>
<p>Our paper is available <a href="https://github.com/CV-H-G-R-group/Long-tailed-Data-Distillation">here</a>.</p>
<h3 id="how-crltdd-works">How CRLT_DD Works</h3>
<p><img style="margin: 0px 40px;" src="CRLT_DD.png" width="800" /></p>



      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/CV-H-G-R-group/Long-tailed-Data-Distillation">CRLT_DD</a> is maintained by <a href="https://github.com/lillian039">Keya Hu</a> and <a href="https://github.com/yijinguo">Yijin Guo</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
